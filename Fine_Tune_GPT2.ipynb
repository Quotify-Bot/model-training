{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fine Tune GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts0mQvccQNiW"
      },
      "source": [
        "# Finetuning GPT2 on a Quotes Dataset\n",
        "#### Dataset: https://github.com/ShivaliGoel/Quotes-500K\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNBNXsrNNcj1"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k39Iur-HLvmX"
      },
      "source": [
        "# Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebCdulIqK7OZ"
      },
      "source": [
        "!wget https://github.com/Quotify-Bot/model-training/releases/download/dataset/quotes_dataset.csv\n",
        "!mkdir models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FYNRAg7OTD1"
      },
      "source": [
        "#Load Dataset\n",
        "df = pd.read_csv('./quotes_dataset.csv')\n",
        "\n",
        "df = df.iloc[:, 0:3]\n",
        "df.columns = ['Quote', 'Author', 'Categories']\n",
        "#\n",
        "df.describe()\n",
        "\n",
        "searchfor = ['inspiration', 'motivation', 'life-lesson', 'love', 'hope', 'friendship', 'life', 'faith', 'universe', 'nature']\n",
        "df = df[df['Categories'].str.contains('|'.join(searchfor),na=False)]\n",
        "df['Length'] = df['Quote'].str.split().apply(len)\n",
        "df.describe()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUVI9uC4L0Ox"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mAIE_x_O2hw"
      },
      "source": [
        "train_test_ratio = 0.85\n",
        "train_valid_ratio = 70/85\n",
        "df_full_train, df_test = train_test_split(df, train_size = train_test_ratio, random_state = 1)\n",
        "# df_train, df_valid = train_test_split(df_full_train, train_size = train_valid_ratio, random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfdhiYf_O6Dg"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install contractions\n",
        "\n",
        "from transformers import AutoTokenizer,AutoModelWithLMHead\n",
        "import contractions\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def prepare_dataset(df, dest_path):\n",
        "    f = open(dest_path, 'w')\n",
        "    data = ''\n",
        "    quotes = df['Quote'].tolist()\n",
        "    for quote in quotes:\n",
        "        #Making all words lower case\n",
        "        quote = str(quote).strip().lower()\n",
        "\n",
        "        #Removing extra whitespaces\n",
        "        quote = re.sub(r\"\\s\", \" \", quote)\n",
        "\n",
        "        #Expand contractions\n",
        "        quote = contractions.fix(quote)\n",
        "\n",
        "        #Filtering out quotes longer than 35 words and less than 10\n",
        "        if(len(quote.split())<10 or len(quote.split())>100):\n",
        "          continue\n",
        "\n",
        "        #Remove non-ASCII characters\n",
        "        encoded_string = quote.encode(\"ascii\", \"ignore\")\n",
        "        quote = encoded_string.decode()\n",
        "\n",
        "        #Remove string with these characters\n",
        "        searchfor = ['~', '-', '--']\n",
        "        match = False\n",
        "        for s in searchfor:\n",
        "          if(s in quote):\n",
        "            match = True\n",
        "            break\n",
        "        if(match):\n",
        "          continue\n",
        "\n",
        "        #Insert whitespace between word and punctuation\n",
        "        # print(quote)\n",
        "        quote = re.sub(r\"([\\w/'+$\\s-]+|[^\\w/'+$\\s-]+)\\s*\", r\"\\1 \", quote)\n",
        "        quote = re.sub(r'<[^>]+>', r\"\",quote)\n",
        "        # print(quote)\n",
        "        data += tokenizer.special_tokens_map['bos_token']+quote+tokenizer.special_tokens_map['eos_token']+'\\n'\n",
        "        \n",
        "    f.write(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkaEXG7hP9As"
      },
      "source": [
        "prepare_dataset(df_full_train, 'train.txt')\n",
        "prepare_dataset(df_test, 'test.txt')\n",
        "# prepare_dataset(df_valid, 'valid.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk8DEOZLQy13"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2') \n",
        "model = AutoModelWithLMHead.from_pretrained('distilgpt2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAw8IvVEL5yz"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdOcbbteTHxC"
      },
      "source": [
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "train_dataset,test_dataset,data_collator = load_dataset('train.txt','test.txt',tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-pyB-mB9OkL"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./models\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=15, # number of training epochs\n",
        "    per_gpu_train_batch_size=32, # batch size for training\n",
        "    per_gpu_eval_batch_size=64,  # batch size for evaluation\n",
        "    logging_steps = 500, # Number of update steps between two evaluations.\n",
        "    save_steps=5000, # after # steps model is saved\n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    )\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    # prediction_loss_only=True,\n",
        "    # compute_metrics=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b72bkNpWMO80"
      },
      "source": [
        "## Use [HyperDash](https://hyperdash.io/) for monitoring and start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6xhWmkz9ZZz"
      },
      "source": [
        "!pip install hyperdash\n",
        "!hyperdash login --email\n",
        "\n",
        "from hyperdash import Experiment\n",
        "\n",
        "exp = Experiment(\"Distil GPT2 Quotes Motivational 10-100, 50 epochs v1\")\n",
        "epochs = exp.param(\"Epochs\", 50)\n",
        "trainer.train()\n",
        "trainer.save_model(\"./models\")\n",
        "tokenizer.save_pretrained(\"./models\")\n",
        "exp.end()\n",
        "\n",
        "from google.colab import files\n",
        "files.download('./models/pytorch_model.bin') \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkTxKoFZLrKg"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl5JebrsGI6f"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import *\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./models\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"./models\")\n",
        "\n",
        "gpt2_finetune = pipeline('text-generation', model=model, tokenizer=tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LWe_KKOKefq"
      },
      "source": [
        "def generate_quote(starting_text, min_length, max_length ):\n",
        "  gen_text = gpt2_finetune (starting_text, min_length = min_length, max_length= max_length, top_k=50, top_p=0.95, temperature=0.5)\n",
        "  return gen_text[0]['generated_text']\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwJznBwa_arz"
      },
      "source": [
        "prompts = ['why am i here',\n",
        "'i am cool and good ',\n",
        "'how are thou',\n",
        "'Life is a journey',\n",
        "'In the end ',\n",
        "'Happiness is '\n",
        "]\n",
        "\n",
        "all_quotes = []\n",
        "\n",
        "for prompt in prompts:\n",
        "  for i in range (10):\n",
        "   all_quotes.append(generate_quote(prompt, 10, 50))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0Nfxyt2q5zU"
      },
      "source": [
        "for q in all_quotes:\n",
        "  print(q)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}